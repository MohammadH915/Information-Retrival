{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647577f0c1ecf16f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reading From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20146dd6d7f42e43",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fb7ccd9dcae1f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c437f65e965567",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_data = read_json_file('./IR_data_news_5k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9d1cb64f1350a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contents = [news_data[i]['content'] for i in news_data]\n",
    "contents_id = [i for i in news_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaaf8b59eb521b2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Document Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41b0ccc4c2d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import FindStems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89333046-5b8e-4623-86b0-019ef0066507",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def tokenize_words(self, doc_string):\n",
    "        token_list = doc_string.strip().split()\n",
    "        token_list = [x.strip(\"\\u200c\") for x in token_list if len(x.strip(\"\\u200c\")) != 0]\n",
    "        return token_list\n",
    "\n",
    "    def tokenize_sentences(self, doc_string):\n",
    "        #finding the numbers\n",
    "        pattern = r\"[-+]?\\d*\\.\\d+|\\d+\"\n",
    "        nums_list = re.findall(pattern, doc_string)\n",
    "        doc_string = re.sub(pattern, 'floatingpointnumber', doc_string)\n",
    "\n",
    "        pattern = r'([!\\.\\?؟]+)[\\n]*'\n",
    "        tmp = re.findall(pattern, doc_string)\n",
    "        doc_string = re.sub(pattern, self.add_tab, doc_string)\n",
    "\n",
    "        pattern = r':\\n'\n",
    "        tmp = re.findall(pattern, doc_string)\n",
    "        doc_string = re.sub(pattern, self.add_tab, doc_string)\n",
    "\n",
    "        pattern = r';\\n'\n",
    "        tmp = re.findall(pattern, doc_string)\n",
    "        doc_string = re.sub(pattern, self.add_tab, doc_string)\n",
    "\n",
    "        pattern = r'؛\\n'\n",
    "        tmp = re.findall(pattern, doc_string)\n",
    "        doc_string = re.sub(pattern, self.add_tab, doc_string)\n",
    "\n",
    "        pattern = r'[\\n]+'\n",
    "        doc_string = re.sub(pattern, self.add_tab, doc_string)\n",
    "\n",
    "        for number in nums_list:\n",
    "            pattern = 'floatingpointnumber'\n",
    "            doc_string = re.sub(pattern, number, doc_string, 1)\n",
    "\n",
    "        doc_string = doc_string.split('\\t\\t')\n",
    "        doc_string = [x for x in doc_string if len(x) > 0]\n",
    "        return doc_string\n",
    "\n",
    "    def add_tab(self, mystring):\n",
    "        mystring = mystring.group()  # this method return the string matched by re\n",
    "        mystring = mystring.strip(' ')  # ommiting the whitespace around the pucntuation\n",
    "        mystring = mystring.strip('\\n') # ommiting the newline around the pucntuation\n",
    "        mystring = \" \" + mystring + \"\\t\\t\"  # adding a space after and before punctuation\n",
    "        return mystring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee441ea459de6d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import re\n",
    "\n",
    "def maketrans(a: str, b: str) -> Dict[int, Any]:\n",
    "    return {ord(a): b for a, b in zip(a, b)}\n",
    "\n",
    "def regex_replace(patterns: str, text: str) -> str:\n",
    "    for pattern, repl in patterns:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def correct_spacing(text):\n",
    "    extra_space_patterns = [\n",
    "            (r\" {2,}\", \" \"),\n",
    "            (r\"\\n{3,}\", \"\\n\\n\"), \n",
    "            (r\"\\u200c{2,}\", \"\\u200c\"),\n",
    "            (r\"\\u200c{1,} \", \" \"), \n",
    "            (r\" \\u200c{1,}\", \" \"), \n",
    "            (r\"\\b\\u200c*\\B\", \"\"), \n",
    "            (r\"\\B\\u200c*\\b\", \"\"),  \n",
    "            (r\"[ـ\\r]\", \"\"),\n",
    "        ]\n",
    "    \n",
    "    text = regex_replace(extra_space_patterns, text)\n",
    "    return text\n",
    "\n",
    "class Normalizer():\n",
    "    def normalize(self, content):\n",
    "        translation_src = \"ؠػػؽؾؿكيٮٯٷٸٹٺٻټٽٿڀځٵٶٷٸٹٺٻټٽٿڀځڂڅڇڈډڊڋڌڍڎڏڐڑڒړڔڕږڗڙښڛڜڝڞڟڠڡڢڣڤڥڦڧڨڪګڬڭڮڰڱڲڳڴڵڶڷڸڹںڻڼڽھڿہۂۃۄۅۆۇۈۉۊۋۏۍێېۑےۓەۮۯۺۻۼۿݐݑݒݓݔݕݖݗݘݙݚݛݜݝݞݟݠݡݢݣݤݥݦݧݨݩݪݫݬݭݮݯݰݱݲݳݴݵݶݷݸݹݺݻݼݽݾݿࢠࢡࢢࢣࢤࢥࢦࢧࢨࢩࢪࢫࢮࢯࢰࢱࢬࢲࢳࢴࢶࢷࢸࢹࢺࢻࢼࢽﭐﭑﭒﭓﭔﭕﭖﭗﭘﭙﭚﭛﭜﭝﭞﭟﭠﭡﭢﭣﭤﭥﭦﭧﭨﭩﭮﭯﭰﭱﭲﭳﭴﭵﭶﭷﭸﭹﭺﭻﭼﭽﭾﭿﮀﮁﮂﮃﮄﮅﮆﮇﮈﮉﮊﮋﮌﮍﮎﮏﮐﮑﮒﮓﮔﮕﮖﮗﮘﮙﮚﮛﮜﮝﮞﮟﮠﮡﮢﮣﮤﮥﮦﮧﮨﮩﮪﮫﮬﮭﮮﮯﮰﮱﺀﺁﺃﺄﺅﺆﺇﺈﺉﺊﺋﺌﺍﺎﺏﺐﺑﺒﺕﺖﺗﺘﺙﺚﺛﺜﺝﺞﺟﺠﺡﺢﺣﺤﺥﺦﺧﺨﺩﺪﺫﺬﺭﺮﺯﺰﺱﺲﺳﺴﺵﺶﺷﺸﺹﺺﺻﺼﺽﺾﺿﻀﻁﻂﻃﻄﻅﻆﻇﻈﻉﻊﻋﻌﻍﻎﻏﻐﻑﻒﻓﻔﻕﻖﻗﻘﻙﻚﻛﻜﻝﻞﻟﻠﻡﻢﻣﻤﻥﻦﻧﻨﻩﻪﻫﻬﻭﻮﻯﻰﻱﻲﻳﻴىكي“” \"\n",
    "        translation_dst = ('یککیییکیبقویتتبتتتبحاوویتتبتتتبحححچدددددددددررررررررسسسصصطعففففففققکککککگگگگگللللنننننهچهههوووووووووییییییهدرشضغهبببببببححددرسعععففکککممنننلررسححسرحاایییووییحسسکببجطفقلمییرودصگویزعکبپتریفقنااببببپپپپببببتتتتتتتتتتتتففففححححححححچچچچچچچچددددددددژژررککککگگگگگگگگگگگگننننننههههههههههییییءاااووااییییااببببتتتتثثثثججججححححخخخخددذذررززسسسسششششصصصصضضضضططططظظظظععععغغغغففففققققککککللللممممننننههههوویییییییکی\"\" ')\n",
    "        suffixes = {\n",
    "                    \"ی\",\n",
    "                    \"ای\",\n",
    "                    \"ها\",\n",
    "                    \"های\",\n",
    "                    \"هایی\",\n",
    "                    \"تر\",\n",
    "                    \"تری\",\n",
    "                    \"ترین\",\n",
    "                    \"گر\",\n",
    "                    \"گری\",\n",
    "                    \"ام\",\n",
    "                    \"ات\",\n",
    "                    \"اش\",\n",
    "                }\n",
    "        replacements = [\n",
    "                    (\"﷽\", \"بسم الله الرحمن الرحیم\"),\n",
    "                    (\"﷼\", \"ریال\"),\n",
    "                    (\"(ﷰ|ﷹ)\", \"صلی\"),\n",
    "                    (\"ﷲ\", \"الله\"),\n",
    "                    (\"ﷳ\", \"اکبر\"),\n",
    "                    (\"ﷴ\", \"محمد\"),\n",
    "                    (\"ﷵ\", \"صلعم\"),\n",
    "                    (\"ﷶ\", \"رسول\"),\n",
    "                    (\"ﷷ\", \"علیه\"),\n",
    "                    (\"ﷸ\", \"وسلم\"),\n",
    "                    (\"ﻵ|ﻶ|ﻷ|ﻸ|ﻹ|ﻺ|ﻻ|ﻼ\", \"لا\"),\n",
    "                ]\n",
    "        number_translation_src = \"0123456789%٠١٢٣٤٥٦٧٨٩\"\n",
    "        number_translation_dst = \"۰۱۲۳۴۵۶۷۸۹٪۰۱۲۳۴۵۶۷۸۹\"\n",
    "        specials_chars_patterns = [\n",
    "            # Remove almoast all arabic unicode superscript and subscript characters in the ranges of 00600-06FF, 08A0-08FF, FB50-FDFF, and FE70-FEFF\n",
    "            (\n",
    "                \"[\\u0605\\u0653\\u0654\\u0655\\u0656\\u0657\\u0658\\u0659\\u065a\\u065b\\u065c\\u065d\\u065e\\u065f\\u0670\\u0610\\u0611\\u0612\\u0613\\u0614\\u0615\\u0616\\u0618\\u0619\\u061a\\u061e\\u06d4\\u06d6\\u06d7\\u06d8\\u06d9\\u06da\\u06db\\u06dc\\u06dd\\u06de\\u06df\\u06e0\\u06e1\\u06e2\\u06e3\\u06e4\\u06e5\\u06e6\\u06e7\\u06e8\\u06e9\\u06ea\\u06eb\\u06ec\\u06ed\\u06fd\\u06fe\\u08ad\\u08d4\\u08d5\\u08d6\\u08d7\\u08d8\\u08d9\\u08da\\u08db\\u08dc\\u08dd\\u08de\\u08df\\u08e0\\u08e1\\u08e2\\u08e3\\u08e4\\u08e5\\u08e6\\u08e7\\u08e8\\u08e9\\u08ea\\u08eb\\u08ec\\u08ed\\u08ee\\u08ef\\u08f0\\u08f1\\u08f2\\u08f3\\u08f4\\u08f5\\u08f6\\u08f7\\u08f8\\u08f9\\u08fa\\u08fb\\u08fc\\u08fd\\u08fe\\u08ff\\ufbb2\\ufbb3\\ufbb4\\ufbb5\\ufbb6\\ufbb7\\ufbb8\\ufbb9\\ufbba\\ufbbb\\ufbbc\\ufbbd\\ufbbe\\ufbbf\\ufbc0\\ufbc1\\ufc5e\\ufc5f\\ufc60\\ufc61\\ufc62\\ufc63\\ufcf2\\ufcf3\\ufcf4\\ufd3e\\ufd3f\\ufe70\\ufe71\\ufe72\\ufe76\\ufe77\\ufe78\\ufe79\\ufe7a\\ufe7b\\ufe7c\\ufe7d\\ufe7e\\ufe7f\\ufdfa\\ufdfb]\",\n",
    "                \"\",\n",
    "            ),\n",
    "        ]\n",
    "        text = content\n",
    "        translations = maketrans(translation_src, translation_dst)\n",
    "        text = text.translate(translations)\n",
    "        translations = maketrans(\n",
    "            number_translation_src,\n",
    "            number_translation_dst,\n",
    "        )\n",
    "        text = text.translate(translations)\n",
    "        diacritics_patterns = [\n",
    "            # remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN\n",
    "            (\"[\\u064b\\u064c\\u064d\\u064e\\u064f\\u0650\\u0651\\u0652]\", \"\"),\n",
    "        ]\n",
    "        text = regex_replace(diacritics_patterns, text)\n",
    "        text = correct_spacing(text)\n",
    "        for old, new in replacements:\n",
    "                text = re.sub(old, new, text)\n",
    "        text = regex_replace(specials_chars_patterns, text)\n",
    "        char_delete = ['،', '.', ')', '(', '}', '{', '«', '»', '؛', ':', '؟', '>', '<', '|', '+', '-', '*', '^', '%', '#', '=', '_', '/', '«', '»', '$', '[', ']', '&', \"❊\", '«', '»', '\"', '!', \"'\"]\n",
    "        alphabet = 'QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm?!.\\xad'\n",
    "        for c in char_delete:\n",
    "            text = text.replace(c, \"\")\n",
    "        for c in alphabet:\n",
    "            text = text.replace(c, \"\")\n",
    "        return text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bcf331f8a224f85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44864fbf2846cb8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_top_tokens(contents, num):\n",
    "    all_tokens = [token for content in contents for token in content]\n",
    "    top_tokens = Counter(all_tokens).most_common(num)\n",
    "\n",
    "    for token, count in top_tokens:\n",
    "        print(f\"{token}: {count} occurrences\")\n",
    "\n",
    "    return [token for token, _ in Counter(all_tokens).most_common(num)]    \n",
    "\n",
    "def get_tokens(content, normalizer, tokenizer, stemmer, stopwords):\n",
    "        # Normalizing\n",
    "        normalized_content = normalizer.normalize(content)\n",
    "       \n",
    "        # Tokenizing\n",
    "        content_tokens = tokenizer.tokenize_words(normalized_content)\n",
    "\n",
    "        # Preprocess tokens\n",
    "        tokens = [\n",
    "            stemmer.convert_to_stem(token)\n",
    "            for token in content_tokens\n",
    "            if not (token in stopwords)\n",
    "        ]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "def preprocess(contents, normalizer, tokenizer, stemmer, stopwords, remove_top_n=50):\n",
    "    preprocessed_docs = []\n",
    "\n",
    "    for content in contents:\n",
    "        preprocessed_docs.append(get_tokens(content, normalizer, tokenizer, stemmer, stopwords))\n",
    "        \n",
    "    top_tokens = get_top_tokens(preprocessed_docs, remove_top_n)\n",
    "    preprocessed_docs = [\n",
    "        [token for token in tokens if token not in top_tokens]\n",
    "        for tokens in preprocessed_docs\n",
    "    ]\n",
    "\n",
    "    return preprocessed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad392a23a45c5c71",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    'و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای',\n",
    "    'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'گفت', 'می‌شود',\n",
    "    'وی', 'شد', 'دارد', 'ما', 'اما', 'یا', 'شده', 'باید', 'هر', 'آنها',\n",
    "    'بود', 'او', 'دیگر', 'دو', 'مورد', 'می‌کند', 'شود', 'کند', 'وجود',\n",
    "    'بین', 'پیش', 'شده_است', 'پس', 'نظر', 'اگر', 'همه', 'یکی', 'حال',\n",
    "    'هستند', 'من', 'کنند', 'نیست', 'باشد', 'چه', 'بی', 'می', 'بخش',\n",
    "    'می‌کنند', 'همین', 'افزود', 'هایی', 'دارند', 'راه', 'همچنین', 'روی',\n",
    "    'داد', 'بیشتر', 'بسیار', 'سه', 'داشت', 'چند', 'سوی', 'تنها', 'هیچ',\n",
    "    'میان', 'اینکه', 'شدن', 'بعد', 'جدید', 'ولی', 'حتی', 'کردن', 'برخی',\n",
    "    'کردند', 'می‌دهد', 'اول', 'نه', 'کرده_است', 'نسبت', 'بیش', 'شما',\n",
    "    'چنین', 'طور', 'افراد', 'تمام', 'درباره', 'بار', 'بسیاری', 'می‌تواند',\n",
    "    'کرده', 'چون', 'ندارد', 'دوم', 'بزرگ', 'طی', 'حدود', 'همان', 'بدون',\n",
    "    'البته', 'آنان', 'می‌گوید', 'دیگری', 'خواهد_شد', 'کنیم', 'قابل',\n",
    "    'یعنی', 'رشد', 'می‌توان', 'وارد', 'کل', 'ویژه', 'قبل', 'براساس', 'نیاز',\n",
    "    'گذاری', 'هنوز', 'لازم', 'سازی', 'بوده_است', 'چرا', 'می‌شوند', 'وقتی',\n",
    "    'گرفت', 'کم', 'جای', 'حالی', 'تغییر', 'پیدا', 'اکنون', 'تحت', 'باعث',\n",
    "    'مدت', 'فقط', 'زیادی', 'تعداد', 'آیا', 'بیان', 'رو', 'شدند', 'عدم',\n",
    "    'کرده_اند', 'بودن', 'نوع', 'بلکه', 'جاری', 'دهد', 'برابر', 'مهم', 'بوده',\n",
    "    'اخیر', 'مربوط', 'امر', 'زیر', 'گیری', 'شاید', 'خصوص', 'آقای', 'اثر',\n",
    "    'کننده', 'بودند', 'فکر', 'کنار', 'اولین', 'سوم', 'سایر', 'کنید', 'ضمن',\n",
    "    'مانند', 'باز', 'می‌گیرد', 'ممکن', 'حل', 'دارای', 'پی', 'مثل', 'می‌رسد',\n",
    "    'اجرا', 'دور', 'منظور', 'کسی', 'موجب', 'طول', 'امکان', 'آنچه', 'تعیین',\n",
    "    'گفته', 'شوند', 'جمع', 'خیلی', 'علاوه', 'گونه', 'تاکنون', 'رسید', 'ساله',\n",
    "    'گرفته', 'شده_اند', 'علت', 'چهار', 'داشته_باشد', 'خواهد_بود', 'طرف', 'تهیه',\n",
    "    'تبدیل', 'مناسب', 'زیرا', 'مشخص', 'می‌توانند', 'نزدیک', 'جریان', 'روند',\n",
    "    'بنابراین', 'می‌دهند', 'یافت', 'نخستین', 'بالا', 'پنج', 'ریزی', 'عالی',\n",
    "    'چیزی', 'نخست', 'بیشتری', 'ترتیب', 'شده_بود', 'خاص', 'خوبی', 'خوب',\n",
    "    'شروع', 'فرد', 'کامل', 'غیر', 'می‌رود', 'دهند', 'آخرین', 'دادن', 'جدی',\n",
    "    'بهترین', 'شامل', 'گیرد', 'بخشی', 'باشند', 'تمامی', 'بهتر', 'داده_است',\n",
    "    'حد', 'نبود', 'کسانی', 'می‌کرد', 'داریم', 'علیه', 'می‌باشد', 'دانست',\n",
    "    'ناشی', 'داشتند', 'دهه', 'می‌شد', 'ایشان', 'آنجا', \"'\", '@', '±','Ø','é','ú','ءامنوا','آارء','آباداناستان','آبادانشمس','آبادسازی','آبادصنعت','آبادمغان','آباد۳۴','آباندر','آبانماه', 'آباکاروف','آببا','آبخواه','آبدی','آبدیدگی','آبرامز','آبرودار','آبرومندی']\n",
    "# stopwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc6022bff5a7fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_docs = preprocess(contents, Normalizer(), Tokenizer(), FindStems(), stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35364897b0c79b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(preprocessed_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8202f974f271034",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Positional indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b283555276be28",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a37f44749ad18",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Term:\n",
    "    def __init__(self):\n",
    "        self.total_frequency = 0\n",
    "        self.positions = defaultdict(list)\n",
    "        self.frequency = defaultdict(int)\n",
    "\n",
    "    def update_posting(self, doc_id, term_pos):\n",
    "        self.positions[doc_id].append(term_pos)\n",
    "        self.frequency[doc_id] += 1\n",
    "        self.total_frequency += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a085944be771b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def positional_indexing(preprocessed_docs):\n",
    "    positional_inverted_indexing = {}\n",
    "\n",
    "    for doc_id, terms in enumerate(preprocessed_docs):\n",
    "        for pos, term in enumerate(terms):\n",
    "            term_obj = positional_inverted_indexing.setdefault(term, Term())\n",
    "            term_obj.update_posting(doc_id, pos)\n",
    "            positional_inverted_indexing[term] = term_obj\n",
    "\n",
    "    return positional_inverted_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9803588602d72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positional_index = positional_indexing(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff418e5e649873",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "for term in positional_index:\n",
    "    dict_list.append((len(positional_index[term].frequency), term))\n",
    "dict_list.sort()\n",
    "print(dict_list[len(positional_index) - 1], dict_list[len(positional_index) - 2], dict_list[len(positional_index) - 3])\n",
    "print(dict_list[0], \":\", positional_index[dict_list[0][1]].frequency)\n",
    "print(dict_list[1], \":\", positional_index[dict_list[1][1]].frequency)\n",
    "print(dict_list[2], \":\", positional_index[dict_list[2][1]].frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab53a8186ad5d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719945266d157e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21be2926252d9f8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_tf(term_frequency):\n",
    "    return 1 + math.log(term_frequency) if term_frequency > 0 else 0\n",
    "\n",
    "def calculate_idf(len_documents, document_count):\n",
    "    return math.log(len_documents / (1 + document_count))\n",
    "\n",
    "def tfidf_vectorizer(documents):\n",
    "    rows, cols, data = [], [], []\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        term_frequencies = Counter(document)\n",
    "        for term, freq in term_frequencies.items():\n",
    "            if term not in unique_terms_dic:\n",
    "                continue\n",
    "            tfidf_weight = calculate_tf(freq) * idf_values[term]\n",
    "            if tfidf_weight != 0:\n",
    "                rows.append(doc_id)\n",
    "                cols.append(unique_terms_dic[term])\n",
    "                data.append(tfidf_weight)\n",
    "\n",
    "    tfidf_matrix = csr_matrix((data, (rows, cols)), shape=(len(documents), len(unique_terms)))\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ba69d0c6952f7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_documents = len(preprocessed_docs)\n",
    "unique_terms = sorted(set(term for document in preprocessed_docs for term in document))\n",
    "unique_terms_dic = {term: index for index, term in enumerate(unique_terms)}\n",
    "idf_values = {term: calculate_idf(len_documents, len(positional_index[term].frequency)) for term in unique_terms}\n",
    "tfidf_matrix = tfidf_vectorizer(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"length of tf_idf vectors:\" ,len(unique_terms))\n",
    "\n",
    "term_max_idf = 'فوتسال'\n",
    "for term in idf_values:\n",
    "    if idf_values[term_max_idf] < idf_values[term]:\n",
    "        term_max_idf = term\n",
    "print(\"term with max idf :\", term_max_idf, \"and idf value is :\", idf_values[term_max_idf])\n",
    "term_min_idf = 'فوتسال'\n",
    "for term in idf_values:\n",
    "    if idf_values[term_min_idf] > idf_values[term]:\n",
    "        term_min_idf = term\n",
    "print(\"term with min idf :\", term_min_idf, \"and idf value is :\", idf_values[term_min_idf])\n",
    "term_max_idf = \"\"\n",
    "max_weight = 0\n",
    "term_min_idf = \"\"\n",
    "min_weight = 100000000000\n",
    "term_frequencies = Counter(preprocessed_docs[0])\n",
    "for term, freq in term_frequencies.items():\n",
    "    if term not in unique_terms_dic:\n",
    "        continue\n",
    "    tfidf_weight = calculate_tf(freq) * idf_values[term]\n",
    "    if tfidf_weight == 0:\n",
    "        continue\n",
    "    if tfidf_weight > max_weight:\n",
    "        max_weight = tfidf_weight\n",
    "        term_max_idf = term\n",
    "    if tfidf_weight < min_weight:\n",
    "        min_weight = tfidf_weight\n",
    "        term_min_idf = term\n",
    "print(f\"for document {contents_id[0]} term with max idf :\", term_max_idf, \"and idf value is :\", idf_values[term_max_idf])\n",
    "print(f\"for document {contents_id[0]} term with min idf :\", term_min_idf, \"and idf value is :\", idf_values[term_min_idf])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9218b276b37fc16a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c35eb527036d5f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(unique_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b0502809f9ee8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Champion List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0f4f1dde7582d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "def create_champion_lists(matrix, champion_list_size): \n",
    "    champion_lists = {}\n",
    "    \n",
    "\n",
    "    for term_index, term in enumerate(unique_terms_dic):\n",
    "        row_indices, data = matrix[:, term_index].nonzero()\n",
    "        champion_lists[term_index] = [(matrix[row_indices[i], term_index], row_indices[i]) for i in range(len(row_indices))]\n",
    "        heapq.heapify(champion_lists[term_index])\n",
    "        champion_lists[term_index] = heapq.nlargest(champion_list_size, champion_lists[term_index])\n",
    "\n",
    "    return champion_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f398736c755be",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "champion_list_size = 5\n",
    "\n",
    "champion_lists = create_champion_lists(tfidf_matrix, champion_list_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819009aac727ad52",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Answer Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37fd13c976e09d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = vector_a.multiply(vector_b).sum()\n",
    "\n",
    "    norm_a = np.linalg.norm(vector_a.toarray())\n",
    "    norm_b = np.linalg.norm(vector_b.toarray())\n",
    "\n",
    "    return dot_product / (norm_a * norm_b) if norm_a > 0 and norm_b > 0 else 0\n",
    "\n",
    "def calculate_cosine_similarities_with_index_elimination(query_vector, document_vectors, k):\n",
    "    similarities_dict = {}\n",
    "\n",
    "    for doc_id, document_vector in enumerate(document_vectors):\n",
    "        similarity = cosine_similarity(query_vector, document_vector)\n",
    "        if similarity > 0:\n",
    "            similarities_dict[doc_id] = similarity\n",
    "\n",
    "    top_k_documents = heapq.nlargest(k, similarities_dict, key=similarities_dict.get)\n",
    "    output = {}\n",
    "    for i in top_k_documents:\n",
    "        output[i] = similarities_dict[i]\n",
    "    return top_k_documents, output\n",
    "\n",
    "def calculate_cosine_similarities_with_champions_list(query_vector, k):\n",
    "    similarities_dict = {}\n",
    "    term_indices = query_vector.getrow(0).indices\n",
    "    for term_index in term_indices:\n",
    "        if term_index in champion_lists:\n",
    "            for doc_weight, doc_id in champion_lists[term_index]:\n",
    "                if doc_id not in similarities_dict:\n",
    "                    similarities_dict[doc_id] = 0\n",
    "                similarities_dict[doc_id] += query_vector[0, doc_id] * doc_weight\n",
    "\n",
    "    top_k_documents = heapq.nlargest(k, similarities_dict, key=similarities_dict.get)\n",
    "    output = {}\n",
    "    for i in top_k_documents:\n",
    "        output[i] = similarities_dict[i]\n",
    "    return top_k_documents, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_cosine_similarities_docs(query_vector, query_id, document_vectors, k):\n",
    "    similarities_dict = {}\n",
    "\n",
    "    for doc_id, document_vector in enumerate(document_vectors):\n",
    "        if doc_id == query_id:\n",
    "            continue\n",
    "        similarity = cosine_similarity(query_vector, document_vector)\n",
    "        if similarity > 0:\n",
    "            similarities_dict[doc_id] = similarity\n",
    "\n",
    "\n",
    "    top_k_documents = heapq.nlargest(k, similarities_dict, key=similarities_dict.get)\n",
    "    output = {}\n",
    "    for i in top_k_documents:\n",
    "        output[i] = similarities_dict[i]\n",
    "    return output\n",
    "\n",
    "print(calculate_cosine_similarities_docs(tfidf_matrix[0],0, tfidf_matrix, k=1))\n",
    "print(contents[1782], contents_id[1782])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a2f8322951a6b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcebfbda132c673",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_query = \"نوواک جوکوویچ مسابقات گرند پری استرالیا\"\n",
    "print(user_query)\n",
    "\n",
    "query_vector = tfidf_vectorizer([get_tokens(user_query, Normalizer(), Tokenizer(), FindStems(), stopwords)])[0]\n",
    "top_k_documents, values = calculate_cosine_similarities_with_index_elimination(query_vector, tfidf_matrix, k=5)\n",
    "for rank, doc_id in enumerate(top_k_documents, start=1):\n",
    "    print(f\"Rank {rank}: Document {contents_id[doc_id]}  \\nContent: {news_data[contents_id[doc_id]]}\\n\")\n",
    "# \n",
    "top_k_documents, values = calculate_cosine_similarities_with_champions_list(query_vector, k=5)\n",
    "for rank, doc_id in enumerate(top_k_documents, start=1):\n",
    "    print(f\"Rank {rank}: Document {contents_id[doc_id]} \\nContent: {news_data[contents_id[doc_id]]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376704020c01c83",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "def storeData():\n",
    "    # database\n",
    "    db = {}\n",
    "    db['Positional_Index'] = positional_index\n",
    "     \n",
    "    with open('./db', 'ab') as dbfile:\n",
    "        # Serialize and store the data using pickle\n",
    "        pickle.dump(db, dbfile)\n",
    " \n",
    "def loadData():\n",
    "    with open('./db', 'rb') as dbfile:\n",
    "        # Deserialize the data using pickle\n",
    "        db = pickle.load(dbfile)\n",
    "        for key, value in db.items():\n",
    "            print(key, '=>', value)\n",
    " \n",
    "storeData()\n",
    "loadData()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
